<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face Detection & Tab Switch Prevention</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            text-align: center;
            margin: 20px;
        }
        #video {
            width: 50%;
            border: 2px solid black;
        }
        #status {
            font-size: 18px;
            font-weight: bold;
            color: red;
            margin-top: 10px;
        }
        button {
            padding: 10px 20px;
            font-size: 16px;
            margin-top: 20px;
            cursor: pointer;
        }
    </style>
</head>
<body>

    <h2>Face Detection & Tab Switch Prevention</h2>
    <video id="video" autoplay></video>
    <p id="status">Initializing...</p>
    <button onclick="startExam()">Start Exam</button>

    <script>
        let video = document.getElementById('video');
        let statusMessage = document.getElementById('status');
        let isSinglePerson = false;

        // ðŸ“· Start Webcam for Face Detection
        async function startCamera() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
                video.play();
                detectFaces();
            } catch (error) {
                console.error('Error accessing webcam:', error);
                statusMessage.innerText = 'Error accessing webcam!';
                alert('Camera access is required for the exam!');
                window.location.href = 'about:blank';
            }
        }

        // ðŸ”Ž Detect Face Presence
        async function detectFaces() {
            const model = await blazeface.load();
            setInterval(async () => {
                const predictions = await model.estimateFaces(video, false);

                if (predictions.length === 1) {
                    statusMessage.innerText = 'âœ… One person detected. Exam can proceed.';
                    statusMessage.style.color = 'green';
                    isSinglePerson = true;
                } else {
                    statusMessage.innerText = 'âŒ Error: Multiple people detected or no one present!';
                    statusMessage.style.color = 'red';
                    isSinglePerson = false;
                    alert('Please ensure only one person is visible on the screen.');
                }
            }, 2000);
        }

        // â–¶ Start Exam After Verification
        function startExam() {
            if (!isSinglePerson) {
                alert('You cannot start the exam until only one person is detected.');
                return;
            }
            alert('Exam Started!');
        }

        // ðŸ”’ Prevent Tab Switching
        document.addEventListener('visibilitychange', () => {
            if (document.hidden) {
                alert('Tab switching is not allowed!');
                window.location.href = 'about:blank';
            }
        });

        // Start Camera on Page Load
        startCamera();
    </script>

</body>
</html>

